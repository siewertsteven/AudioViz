<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8" />
	<title>Web Audio Visualizer</title>
	<style>
	body {
         background: #eeeeee;
         font-family: tahoma, verdana, sans serif;
      }

      canvas {
        margin-left:10px;
        margin-top:10px;
        box-shadow: 4px 4px 8px rgba(0,0,0,0.5);
        background: black;
    }
      
      #controls{
      	margin-left:10px;
        margin-top:10px;
      }
	</style>
	<script>
	// An IIFE ("Iffy") - see the notes in mycourses
	(function(){
		"use strict";
		
		var NUM_SAMPLES = 256;
		var SOUND_1 = 'media/RulesOfNature.mp3';
		var SOUND_2 = 'media/HotWindBlowing.mp3';
		var SOUND_3 = 'media/CollectiveConciousness.mp3';
		var SOUND_3 = 'media/RedSun.mp3';
		var audioElement;
		var analyserNode;
		var canvas,ctx;
		var maxRadius = 50;
		var invert = false, tintRed = false, noise=false, lines=false;
		//images
		var stars, sun, tree;
		
		function init(){
			// set up canvas stuff
			canvas = document.querySelector('canvas');
			ctx = canvas.getContext("2d");
			
			
			// get reference to <audio> element on page
			audioElement = document.querySelector('audio');
			
			// call our helper function and get an analyser node
			analyserNode = createWebAudioContextWithAnalyserNode(audioElement);
			
			// get sound track <select> and Full Screen button working
			setupUI();
			
			// load and play default sound into audio element
			playStream(audioElement,SOUND_1);
			
			
			
			tree = document.getElementById("palmTree");
			stars = document.getElementById("stars");
			sun = document.getElementById("sun");
			// start animation loop
			update();
		}
		
		
		function createWebAudioContextWithAnalyserNode(audioElement) {
			var audioCtx, analyserNode, sourceNode;
			// create new AudioContext
			// The || is because WebAudio has not been standardized across browsers yet
			// http://webaudio.github.io/web-audio-api/#the-audiocontext-interface
			audioCtx = new (window.AudioContext || window.webkitAudioContext);
			
			// create an analyser node
			analyserNode = audioCtx.createAnalyser();
			
			/*
			We will request NUM_SAMPLES number of samples or "bins" spaced equally 
			across the sound spectrum.
			
			If NUM_SAMPLES (fftSize) is 256, then the first bin is 0 Hz, the second is 172 Hz, 
			the third is 344Hz. Each bin contains a number between 0-255 representing 
			the amplitude of that frequency.
			*/ 
			
			// fft stands for Fast Fourier Transform
			analyserNode.fftSize = NUM_SAMPLES;
			
			// this is where we hook up the <audio> element to the analyserNode
			sourceNode = audioCtx.createMediaElementSource(audioElement); 
			sourceNode.connect(analyserNode);
			
			// here we connect to the destination i.e. speakers
			analyserNode.connect(audioCtx.destination);
			return analyserNode;
		}
		function manipulatePixels(){
			var imageData = ctx.getImageData(0,0,canvas.width,canvas.height);
			var data = imageData.data;
			var length = data.length;
			var width = imageData.width;
			
			for (var i=0; i<length -4; i+=4){
			if(tintRed){
					
					data[i] = data[i] + 100;
			}
			if(invert){
				var red = data[i]
				var green = data[i+1]
				var blue = data[i+2];
				data[i] = 255 - red;
				data[i+1] = 255 - green;
				data[i+2] = 255 - blue;
			}
			if (noise && Math.random()<.10){
				data[i] = data[i + 1] = data[i + 2] = 128;
			}
			
			if (lines){
				var row = Math.floor(i/4/width);
				if (row % 50 == 0){
					data[i] = data[i+1] = data[i+2] = data[i+3] = 255;
					
					data[i+(width*4)] = data[i+(width*4)+1] = data[i+(width*4)+2]= data[i+(width*4)+3] = 255;
				}
			
			}
			}
	
			
			ctx.putImageData(imageData, 0, 0);
		}
		
		function setupUI(){
			document.querySelector("#trackSelect").onchange = function(e){
				playStream(audioElement,e.target.value);
			};
			
			document.querySelector("#radiusSlider").onchange = function(e){
			  maxRadius = 50 + (e.target.value)/1; console.log(e.target.value);
			}
			
			document.querySelector("#fsButton").onclick = function(){
				requestFullscreen(canvas);
			};
			
			
		}
		
		function playStream(audioElement,path){
			audioElement.src = path;
			audioElement.play();
			audioElement.volume = 0.2;
			document.querySelector('#status').innerHTML = "Now playing: " + path;
		}
		
		function update() { 
			// this schedules a call to the update() method in 1/60 seconds
			requestAnimationFrame(update);
			/*
				Nyquist Theorem
				http://whatis.techtarget.com/definition/Nyquist-Theorem
				The array of data we get back is 1/2 the size of the sample rate 
			*/
			
			// create a new array of 8-bit integers (0-255)
			var data = new Uint8Array(NUM_SAMPLES/2); 
			
			// populate the array with the frequency data
			// notice these arrays can be passed "by reference" 
			analyserNode.getByteFrequencyData(data);
		
			// OR
			//analyserNode.getByteTimeDomainData(data); // waveform data
			
			// DRAW!
			ctx.clearRect(0,0,800,600);  
			var barWidth = 5;
			var barSpacing = 1;
			var barHeight = 100;
			var topSpacing = 100;
			
			// loop through the data and draw!
			for(var i=0; i<data.length; i++) { 

			}
			ctx.drawImage(stars, 0, 0, 960, 540);
			
			ctx.drawImage(sun, 0, 0, 960, 540);
		
			ctx.save();
			ctx.globalCompositeOperation = "source-in";
			ctx.fillStyle="yellow";
			ctx.fillRect(0,0,960,540);
			ctx.restore();
			
			ctx.drawImage(tree, 0, 0, 960, 540);
			

			ctx.save();
			ctx.globalCompositeOperation = "source-in";
			ctx.fillStyle="red";
			ctx.fillRect(0,0,960,540);
			ctx.restore();
			
			ctx.save();
			ctx.globalCompositeOperation = "darken";
			ctx.drawImage(tree, 0, 0, 960, 540);
			ctx.restore();
			
			
			
			manipulatePixels();
			
			invert = document.getElementById("invert").checked;
			tintRed = document.getElementById("tintRed").checked;
			noise = document.getElementById("noise").checked;
			lines = document.getElementById("lines").checked;
		} 
		
		// HELPER
		function makeColor(red, green, blue, alpha){
   			var color='rgba('+red+','+green+','+blue+', '+alpha+')';
   			return color;
		}
		
		 // FULL SCREEN MODE
		function requestFullscreen(element) {
			if (element.requestFullscreen) {
			  element.requestFullscreen();
			} else if (element.mozRequestFullscreen) {
			  element.mozRequestFullscreen();
			} else if (element.mozRequestFullScreen) { // camel-cased 'S' was changed to 's' in spec
			  element.mozRequestFullScreen();
			} else if (element.webkitRequestFullscreen) {
			  element.webkitRequestFullscreen();
			}
			// .. and do nothing if the method is not supported
		};
		
		
		window.addEventListener("load",init);
	}());
		
	</script>
</head>
<body>
	<canvas id="canvas" width="960" height="540"></canvas>
	<div id="controls">
		<audio controls loop></audio>
		<label>Track: 
			<select id="trackSelect" >
				<option value="media/RulesOfNature.mp3">Rules Of Nature</option>
				<option value="media/HotWindBlowing.mp3">Hot Wind Blowing</option>
				<option value="media/CollectiveConciousness.mp3">Collective Conciousness</option>
				<option value="media/RedSun.mp3">Red Sun</option>
			</select>
		</label>
		<img src="media/Tree.png" id="palmTree" style="display:none;"></img>
		<img src="media/Sun.png" id="sun" style="display:none;"></img>
		<img src="media/Stars.png" id="stars" style="display:none;"></img>
		<br>
		<input type="range" name="circleRadius" id="radiusSlider">Radius Slider
		<br>
		<input type="checkbox" name="tint red" id="tintRed" >Red Tint
		<input type="checkbox" name="invert" id="invert">Invert
		<input type="checkbox" name="noise" id="noise">Noise
		<input type="checkbox" name="lines" id="lines">Lines
		<br>
		<button id="fsButton">Go Full Screen</button><br>
		
		<p id="status">???</p>
	</div>
</body>
</html>
